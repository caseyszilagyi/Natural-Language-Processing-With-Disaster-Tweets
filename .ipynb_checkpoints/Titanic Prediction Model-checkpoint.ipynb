{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for training and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "\n",
    "\n",
    "train_set = pd.read_csv(\"/Users/cszilagyi2019/Desktop/All Coding/Titanic Predictions/train.csv\")\n",
    "test_set = pd.read_csv(\"/Users/cszilagyi2019/Desktop/All Coding/Titanic Predictions/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up Training/Validation/Testing through the dataloaders\n",
    "\n",
    "#Hyperparameters\n",
    "batch_size = 1\n",
    "validation_split = 0.2\n",
    "num_epochs = 20\n",
    "\n",
    "# Create validation split by taking a percentage of the training set:\n",
    "dataset_size = len(train_set)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Use the SubsetRandomSampler to randomly sample the training dataset for \n",
    "# training and validation data. We will feed this into the dataloader below.\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# Create dataloader objects - will be used during training and inference\n",
    "# to iterate over the data.\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, \\\n",
    "                                           sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, \\\n",
    "                                         sampler=val_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, \\\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Neural Network Class\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, num_classes):\n",
    "\n",
    "    super(Net, self).__init__()\n",
    "\n",
    "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "    self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "    self.output = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    # Applying activation function\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    \n",
    "  #Forward pass\n",
    "  def forward(self, x):\n",
    "\n",
    "    out = self.fc1(x)\n",
    "    out = self.sigmoid(out)\n",
    "    out = self.fc2(out)\n",
    "    out = self.sigmoid(out)\n",
    "    out = self.output(out)\n",
    "    #Will use cross entropy\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the dataset\n",
    "\n",
    "class myDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, file):\n",
    "        \n",
    "        #Reading the file into variables\n",
    "        self.x = file.iloc[:, [2,4,5,6,7,9,11]]\n",
    "        self.y = file.iloc[1:892, 1]\n",
    "        \n",
    "        mapping = {'male': 0, 'female': 1, 'S': 0, 'C': 1, \"Q\": 2}\n",
    "        self.x.replace({'Sex': mapping, 'Embarked': mapping})\n",
    "        \n",
    "        self.x = torch.tensor(self.x)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x.iloc[idx], self.y.iloc[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not determine the shape of object type 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e86a76351270>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#Making the datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-4f1b6f2025bb>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Sex'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Embarked'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not determine the shape of object type 'DataFrame'"
     ]
    }
   ],
   "source": [
    "#Setting up Training/Validation/Testing through the dataloaders\n",
    "\n",
    "#Hyperparameters\n",
    "batch_size = 1\n",
    "validation_split = 0.2\n",
    "num_epochs = 20\n",
    "\n",
    "# Create validation split by taking a percentage of the training set:\n",
    "dataset_size = len(train_set)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Use the SubsetRandomSampler to randomly sample the training dataset for \n",
    "# training and validation data. We will feed this into the dataloader below.\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "\n",
    "#Making the datasets\n",
    "train_dataset = myDataSet(train_set)\n",
    "\n",
    "\n",
    "# Create dataloader objects - will be used during training and inference\n",
    "# to iterate over the data.\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \\\n",
    "                                         sampler=val_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \\\n",
    "                                         sampler=val_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, \\\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the neural network\n",
    "net = Net(7, 10, 2)\n",
    "\n",
    "#Cross Entropy Loss/Adam Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7f851d4bd7c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurvived\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "\n",
    "running_loss = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "  for i, (data, survived) in enumerate(train_loader):\n",
    "    print(data)\n",
    "    # Clear the gradients before performing backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    x_train = torch.tensor(data)\n",
    "    \n",
    "    # Perform the forward pass - this call of net calls the forward() fn\n",
    "    outputs = net(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Compute loss on the output of the forward pass and desired label\n",
    "    loss = criterion(outputs, labels)\n",
    "    # Compute the gradients with respect to the loss function\n",
    "    loss.backward()      \n",
    "    # Update the weights in the neural network using the optimizer/backprop\n",
    "    optimizer.step()\n",
    "\n",
    "    # Track losses for plotting later\n",
    "    loss_tracker.append(loss.data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting accuracy, adapted from: \n",
    "# https://towardsdatascience.com/a-simple-starter-guide-to-build-a-neural-network-3c2cf07b8d7c\n",
    "\n",
    "def get_accuracy(loader, my_net):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  for images, labels in loader:\n",
    "      images = Variable(images.view(-1, 28*28))\n",
    "      outputs = my_net(images)\n",
    "      _, predicted = torch.max(outputs.data, 1)  \n",
    "      total += labels.size(0)               \n",
    "      correct += (predicted == labels).sum()\n",
    "\n",
    "  return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
